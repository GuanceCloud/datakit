
# DataKit Master Configuration
---

The DataKit master configuration is used to configure the running behavior of the DataKit itself.

<!-- markdownlint-disable MD046 -->
=== "host deployment"

    Its directory is generally located in:
    
    - Linux/Mac: `/usr/local/datakit/conf.d/datakit.conf`
    - Windows: `C:\Program Files\datakit\conf.d\datakit.conf`

=== "Kubernetes"

    When DaemonSet is installed, **the DataKit does not actually load the configuration**, although this file exists in the corresponding directory. These matches are generated by [injecting environment variables](datakit-daemonset-deploy.md#using-k8-env). For all of the following configurations, you can find the [corresponding environment variable](datakit-daemonset-deploy.md#using-k8-env) configuration in the Kubernetes deployment documentation.
<!-- markdownlint-enable -->

## DataKit Main Configure Sample {#maincfg-example}

DataKit main configure is `datakit.conf`, here is the example sample({{ .Version }}):

<!-- markdownlint-disable MD046 -->
??? info "`datakit.conf`"

    ```toml
    {{ CodeBlock .DatakitConfSample 4 }}
    ```
<!-- markdownlint-enable -->

## Configuration of HTTP Service {#config-http-server}

DataKit opens an HTTP service to receive external data or provide basic data services to the outside world.

<!-- markdownlint-disable MD046 -->
=== "`datakit.conf`"

    ### Modify the HTTP Service Address {#update-http-server-host}
    
    The default HTTP service address is `localhost:9529`, and if port 9529 is occupied, or you want to access the HTTP service of DataKit from outside (for example, you want to receive [RUM](rum.md) or [Tracing](datakit-tracing.md) data), you can modify it to:
    
    ```toml
    [http_api]
       listen = "0.0.0.0:<other-port>"
       # or using IPV6 address
       # listen = "[::]:<other-port>"
    ```

    > NOTE: IPv6 need DataKit [version 1.5.7](changelog.md#cl-1.5.7-new).
    
    #### Using Unix Domain Socket {#uds}
    
    DataKit supports UNIX domain sockets access. Open it as follows: The `listen` field is configured to <b>the full path to a file that does not exist</b>. Here, for example, sock` can be any file name.
    ```toml
    [http_api]
       listen = "/tmp/datakit.sock"
    ```
    After the configuration is complete, you can use the `curl` command to test whether the configuration is successful: `sudo curl --no-buffer -XGET --unix-socket /tmp/datakit.sock http:/localhost/v1/ping`. For more information on the test commands for `curl`, see [here](https://superuser.com/a/925610){:target="_blank"}.
    
    ### HTTP Request Frequency Control {#set-http-api-limit}

    > [:octicons-tag-24: Version-1.62.0](changelog.md#cl-1.62.0) default enabled this limit.
    
    Since DataKit needs to receive a large amount of external data writing, in order to avoid imposing a huge overhead on the node where it is located, DataKit has set a QPS limit of 20 requests per second for the API by default:

    ```toml
    [http_api]
      request_rate_limit = 20.0 # Limits the QPS of requests initiated by each client (IP + API route)
      # If there is indeed a large amount of data to be written, we can appropriately increase the limit
      # to avoid data loss (clients will receive an HTTP 429 error code when the request limit is exceeded).
    ```
    
    ### Other Settings {#http-other-settings}
    
    ```toml
    [http_api]
        close_idle_connection = true # Close idle connections
        timeout = "30s"              # Set server-side HTTP timeout
    ```

=== "Kubernetes"

    See [here](datakit-daemonset-deploy.md#env-http-api).
<!-- markdownlint-enable -->

### HTTP API Access Control {#public-apis}

[:octicons-tag-24: Version-1.64.0](changelog.md#cl-1.64.0)

For security reasons, DataKit restricts the access to some of its own APIs by default (these APIs can only be accessed via localhost). If DataKit is deployed in a public network environment and you need to request these APIs from other machines or the public network, you can modify the configuration of the following `public_apis` field in *datakit.conf*:

```toml
[http_api]
  public_apis = [
    # Allow access to the DataKit's own metric exposure interface /metrics
    "/metrics",

    # ... Other interfaces
  ]
```

By default, `public_apis` is empty. For the sake of convenience and compatibility, only [specific APIs](`apis.md`) are public by default, and all other APIs disabled from external access. For the APIs corresponding to collectors, such as tracing collectors, once the collector is enabled, its APIs will be automatically opened and can be accessed externally by default.

For adding an API whitelist in Kubernetes, please refer to [here](datakit-daemonset-deploy.md#env-http-api).

<!-- markdownlint-disable MD046 -->
???+ warning

    Once `public_apis` is not empty, the APIs that are enabled by default need to be **manually added again**:

    ```toml
    [http_api]
      public_apis = [
        "/v1/write/metric",
        "/v1/write/logging",
        # ...
      ]
    ```
<!-- markdownlint-enable -->

## Global Tag Modification {#set-global-tag}

[:octicons-tag-24: Version-1.4.6](changelog.md#cl-1.4.6)

DataKit allows you to configure global tags for all collected data. Global tags are divided into two categories:

- Global Host Tags(GET): Collected data is bound to the current host, such as CPU/memory metrics.
- Global Election Tags(GET): Collected data comes from a common (remote) entity, such as MySQL/Redis, which generally participates in elections. Therefore, these data will not carry tags related to the current host.

```toml
[global_host_tags] # These are referred to as 'Global Host Tags'
  ip   = "__datakit_ip"
  host = "__datakit_hostname"

[election]
  [election.tags] # These are referred to as 'Global Election Tags'
    project = "my-project"
    cluster = "my-cluster"
```

When adding global tags, there are several points to note:

1. The values of these global tags can use several wildcards currently supported by DataKit (both the double underscore (`__`) prefix and `$` are acceptable):

    1. `__datakit_ip/$datakit_ip`: The tag value will be set to the first primary network card IP obtained by DataKit.
    1. `__datakit_hostname/$datakit_hostname`: The tag value will be set to the hostname of DataKit.

1. Due to [DataKit Data Transmission Protocol restrictions](apis.md#lineproto-limitation), do not include any metrics (Field) fields in the global tags, as this will lead to data processing failure due to protocol violation. For specific details, refer to the field list of the specific collector. Of course, do not add too many tags, and there are also restrictions on the length of each tag's Key and Value.
1. If the collected data already contains a tag with the same name, DataKit will not append the configured global tag.
1. Even if no configuration is specified in GHT, DataKit will still add a `host=__datakit_hostname` tag to GHT. This is because `host` is currently the default field for data connection. Therefore, all collected data, including logs, CPU, and memory metrics, will include the `host` tag.
1. These two types of global tags (GHT/GET) can intersect, such as setting a tag of `project = "my-project"` in both.
1. When no election is enabled, GET follows all tags in GHT (which has at least a `host` tag).
1. Election-based collectors default to appending GET, and non-election-based collectors default to appending GHT.

<!-- markdownlint-disable MD046 -->
???+ tip "How to distinguish between election and non-election collectors?"

    In the collector documentation, there is an identifier similar to the following at the top, which indicates the platform adaptation and collection characteristics of the current collector:

    :fontawesome-brands-linux: :fontawesome-brands-windows: :fontawesome-brands-apple: :material-kubernetes: :material-docker:  · :fontawesome-solid-flag-checkered:

    If it has :fontawesome-solid-flag-checkered:, it means that the current collector is an election-based collector.
<!-- markdownlint-enable -->

### Settings of Global Tag in Remote Collection {#notice-global-tags}

Because DataKit will append the label  `host=<host name where DataKit is located>` to all collected data by default, but this default appended `host` will cause trouble in some cases.

Take MySQL as an example, if MySQL is not on the DataKit machine, but you want this `host` tag to be the real hostname of MySQL being collected (or other identification fields of the cloud database), not the hostname of DataKit.

In this case, we can bypass the global tag on DataKit in two ways:

- In the specific collector, there is generally the following configuration, and we can add a Tag here. For example, if we don't want DataKit to add the Tag `host=xxx` by default, we can overwrite this Tag here, taking MySQL as an example:

```toml
[[inputs.mysql.tags]]
  host = "real-mysql-host-name" 
```

- When you [push data to the DataKit as an HTTP API](apis.md#api-v1-write), you can mask all global tags with the API parameter `ignore_global_tags`.

<!-- markdownlint-disable MD046 -->
???+ info

    Starting from version [1.4.20](changelog.md#cl-1.4.20), DataKit defaults to using the IP/Host from the connection address of the collected service as the value for the `host` tag.
<!-- markdownlint-enable -->

## DataKit Own Running Log Configuration {#logging-config}

DataKit has two own logs, one is its own run log（*/var/log/datakit/log*）and the other is HTTP Access log（*/var/log/datakit/gin.log*）.

The default logging level for DataKit is `info`. Edit `datakit.conf` to modify the log level and slice size:

```toml
[logging]
  level = "debug" # correct info to debug
  rotate = 32     # each log slice is 32MB
```

- `level`: When you set it to `debug`, you can see more logs (currently only the `debug/info` levels are supported).
- `rotate`: DataKit slices the log by default. The default slice size is 32MB, and there are 6 slices in total (1 current write slice plus 5 cut slices, and the number of slices is not yet supported). If you dislike that DataKit logs take up too much disk space (maximum 32 x 6 = 192MB), reduce the `rotate` size (for example, change it to 4 in MB). HTTP access logs are automatically cut in the same way.

## Advanced Configuration {#advance-config}

The following content involves some advanced configuration. If you are not sure about the configuration, it is recommended to consult our technical experts.

### Time Calibration {#ntp}

[:octicons-tag-24: Version-1.75.0](../datakit/changelog.md#cl-1.75.0)

To prevent data collection issues caused by local time deviations, DataKit can detect significant time discrepancies by calling the DataWay interface ([:octicons-tag-24: Version-1.6.0](../deployment/dataway-changelog.md#cl-1.6.0)). When a significant deviation is detected, DataKit will calibrate its current time (without modifying the system time) for data collection purposes.

The following configuration is available in *datakit.conf*:

```toml
  # Use DataWay as the NTP server
  [dataway.ntp]
    enable   = true  # default enabled
    interval = "5m"  # Sync with DataWay time every 5 minutes (minimum 1 minute)

    # If the absolute difference between DataKit time and DataWay time is >= diff, DataKit will adjust data point times using DataWay time.
    diff     = "30s"  # Minimum 5 seconds
```

### IO Module Parameter Adjustment {#io-tuning}

[:octicons-tag-24: Version-1.4.8](changelog.md#cl-1.4.8) ·
[:octicons-beaker-24: Experimental](index.md#experimental)

<!-- markdownlint-disable MD046 -->
=== "`datakit.conf`"

    In some cases, the data collection amount of DataKit is very large. If the network bandwidth is limited, some data collection may be interrupted or discarded. You can mitigate this problem by configuring some parameters of the io module:
    
    ```toml
    [io]
      feed_chan_size  = 1     # length of compact queue
      max_cache_count = 1000  # data bulk sending points, beyond which sending is triggered in the cache
      flush_interval  = "10s" # threshold for sending data at least once every 10s
      flush_workers   = 0     # upload workers, default is the limited CPU-core * 2
    ```
    
    See [corresponding description in k8s](datakit-daemonset-deploy.md#env-io) for blocking mode

=== "Kubernetes"

    See [here](datakit-daemonset-deploy.md#env-io)
<!-- markdownlint-enable -->

### Resource Limit  {#resource-limit}

Because the amount of data processed on the DataKit cannot be estimated, if the resources consumed by the DataKit are not physically limited, it may consume a large amount of resources of the node where it is located. Here we can limit it with the help of cgroup in Linux or job object in Windows, which has the following configuration in *datakit.conf*:

```toml
[resource_limit]
  path = "/datakit" # Linux cgroup restricts directories, such as /sys/fs/cgroup/memory/datakit, /sys/fs/cgroup/cpu/datakit

  # Maximum CPU utilization allowed (percentile)
  cpu_max = 20.0

  # Allows 4GB of memory (memory + swap) by default
  # If set to 0 or negative, memory limits are not enabled
  mem_max_mb = 4096 
```

If the DataKit exceeds the memory limit, it will be forcibly killed by the operating system. The following results can be seen through the command, and the service needs to [be started manually](datakit-service-how-to.md#when-service-failed) at this time.

```shell
$ systemctl status datakit 
● datakit.service - Collects data and upload it to DataFlux.
     Loaded: loaded (/etc/systemd/system/datakit.service; enabled; vendor preset: enabled)
     Active: activating (auto-restart) (Result: signal) since Fri 2022-02-30 16:39:25 CST; 1min 40s ago
    Process: 3474282 ExecStart=/usr/local/datakit/datakit (code=killed, signal=KILL)
   Main PID: 3474282 (code=killed, signal=KILL)
```

<!-- markdownlint-disable MD046 -->
???+ attention

    - resource restriction will only be turned on by default during [host installation](datakit-install.md).
    - resource limit only supports CPU usage and memory usage (mem + swap) controls, and only supports Linux and Windows ([:octicons-tag-24: Version-1.15.0](changelog.md#cl-1.15.0)) operating systems.
    - CPU usage controls is not supported in these windows systems: Windows 7, Windows Server 2008 R2, Windows Server 2008, Windows Vista, Windows Server 2003 and Windows XP.
    - When adjusting resource limit as a non-root user, it is essential to reinstall the service.
    - The CPU core count directly influences the configuration of worker threads in certain DataKit submodules. These worker threads, which handle specific tasks like data uploads, are typically set to a quantity that is a multiple of the total CPU cores. For instance, the data upload worker is commonly configured to be twice the number of CPU cores. Given that each individual upload worker consumes a default of 10MB of memory for data transmission, allocating a substantial number of CPU cores can lead to a significant increase in DataKit's overall memory footprint.

???+ tip

    DataKit supports cgroup V2 from version [1.5.8](changelog.md#cl-1.5.8). If you are unsure of the cgroup version, you can use this command `mount | grep cgroup` to check.
<!-- markdownlint-enable -->

### Election Configuration {#election}

See [here](election.md#config)

### Dataway Settings {#dataway-settings}

Dataway got following settings to be configured:

- `timeout`: The timeout for request data to Dataway. The default value is 30s
- `max_retry_count`: Sets the number of retries to request Dataway (1 by default, max retry is 10) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `retry_delay` : Set the basic step of the retry interval. The default value is 200ms. The so-called basic step is 200ms for the first time, 400ms for the second time, 800ms for the third time, and so on (in increments of $2^n$) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `max_raw_body_size`: Set the maximum size of a single uploaded package (before compression), in bytes [:octicons-tag-24: Version-1.17.1](changelog.md#cl-1.17.1)
- `content_encoding` : v1 or v2 can be selected [:octicons-tag-24: Version-1.17.1](Changelog.md #cl-1.17.1)
    - v1 is line-protocol (default: v1)
    - v2 is the Protobuf protocol. Compared with v1, it has better performance in all aspects

See [here](datakit-daemonset-deploy.md#env-dataway) for configuration under Kubernetes.

#### WAL Queue Configuration {#dataway-wal}

[:octicons-tag-24: Version-1.60.0](changelog.md#cl-1.60.0)

WAL is used to cache data that DataKit cannot upload in time. When there is a sudden large amount of data collection and the data cannot be sent in time, DataKit will write it into a disk queue to avoid blocking data collection and affecting data real-time performance.

The WAL disk queue has a default disk size limit. When the cached data volume exceeds this limit, newly collected data cannot be written and will be discarded. If you do not want to drop such data, you can configure the data type (generally logging `L`) in the `no_drop_categories` list. In this case, the data will not be actively dropped, but it will block data collection.

In the `[dataway.wal]` section, we can adjust the configuration of the WAL queue:

```toml
  [dataway.wal]
     max_capacity_gb = 2.0             # 2GB reserved disk space for each category (M/L/O/T/...)
     workers = 0                       # flush workers on WAL (default to CPU limited cores)
     mem_cap = 0                       # in-memory queue capacity (default to CPU limited cores)
     fail_cache_clean_interval = "30s" # duration for cleaning failed uploaded data
     #no_drop_categories = ["L"]       # category list that do not drop data if WAL disk full
```

The disk files are located in the *cache/dw-wal* directory under the DataKit installation directory:

```shell
/usr/local/datakit/cache/dw-wal/
├── custom_object
│   └── data
├── dialtesting
│   └── data
├── dynamic_dw
│   └── data
├── fc
│   └── data
├── keyevent
│   └── data
├── logging
│   ├── data
│   └── data.00000000000000000000000000000000
├── metric
│   └── data
├── network
│   └── data
├── object
│   └── data
├── profiling
│   └── data
├── rum
│   └── data
├── security
│   └── data
└── tracing
    └── data

13 directories, 14 files
```

Here, except for the *fc* directory, which is the failure retry queue, the other directories correspond to different data types. When data upload fails, these data will be cached in the *fc* directory, and DataKit will periodically upload them later.

If the current host's disk performance is insufficient, you can try using [WAL with tmpfs](wal-tmpfs.md).

### Dataway Sinker {#dataway-sink}

See [here](../deployment/dataway-sink.md)

### Managing DataKit Configuration with Git {#using-gitrepo}

Because the configuration of various collectors in DataKit is text type, it takes a lot of energy to modify and take effect one by one. Here we can use Git to manage these configurations, with the following advantages:

- Automatically synchronize the latest configuration from the remote Git repository and take effect automatically.
- Git has its own version management, which can effectively track the change history of various configurations.

When you install DataKit（supported by [DaemonSet installation](datakit-daemonset-deploy.md) and [host installation](datakit-install.md#env-gitrepo)), you can specify the Git configuration repository.

#### Manually Configure Git Administration {#setup-gitrepo}

DataKit supports the use of git to manage collector configurations, Pipeline, and Python scripts. In *datakit.conf*, find the *git_repos* location and edit the following:

```toml
[git_repos]
  pull_interval = "1m" # Synchronize configuration interval, that is, synchronize once every 1 minute

  [[git_repos.repo]]
    enable = false   # Do not enable the repo

    ###########################################
    # Three protocols supported by Git address: http/git/ssh
    ###########################################
    url = "http://username:password@github.com/path/to/repository.git"

    # The following two protocols (git/ssh) need to be configured with key-path and key-password
    # url = "git@github.com:path/to/repository.git"
    # url = "ssh://git@github.com:9000/path/to/repository.git"
    # ssh_private_key_path = "/Users/username/.ssh/id_rsa"
    # ssh_private_key_password = "<YOUR-PASSSWORD>"

    branch = "master" # Specify git branch
```

Note: After Git synchronization is turned on, the collector configuration in the original `conf.d` directory will no longer take effect (except *datakit.conf* ).

#### Applying Git-managed Pipeline Sample {#gitrepo-example}

We can add Pipeline to the collector configuration to cut the logs of related services. When Git synchronization is turned on, both **the Pipeline that comes with DataKit and the Pipeline synchronized by Git can be used**. In the configuration of [Nginx collector](../integrations/nginx.md), a configuration example of Pipeline.

```toml
[[inputs.nginx]]
    ...
    [inputs.nginx.log]
    ...
    pipeline = "my-nginx.p" # Where to load my-nginx.p, see the "constraint" description below
```

#### Git-managed Usage Constraints {#gitrepo-limitation}

The following constraints must be followed when using Git:

- Create a new `conf.d` folder in git repo, and put the DataKit collector configuration below
- Create a new `pipeline` folder in git repo, and place the Pipeline file below
- Create a new `python.d` folder in git repo, and place the Python script file below

The following is illustrated by legend:

```shell
datakit root directory
├── conf.d
├── data
├── pipeline # top-level Pipeline script
├── python.d # top-level python.d script
├── externals
└── gitrepos
    ├── repo-1  # warehouse 1
    │   ├── conf.d    # dedicated to store collector configuration
    │   ├── pipeline  # dedicated to storing pipeline cutting scripts
    │   │   └── my-nginx.p # legal pipeline script
    │   │   └── 123     # illegal Pipeline subdirectory, and the files under it will not take effect
    │   │       └── some-invalid.p
    │   └── python.d    store python.d scripts
    │       └── core
    └── repo-2  # warehouse 2
        ├── ...
```

The lookup priority is defined as follows:

1. Find the specified file names one by one in the *git_repos* order configured in `datakit.conf` (it is an array that can configure multiple Git repositories), and return the first one if found. For example, look for *my-nginx.p*. If it is found under *pipeline* in the first repository directory, it will prevail. **Even if there is *my-nginx.p* with the same name in the second repository, it will not be selected.**。

2. If not found in *git_repos* , go to the *<DataKit Installation Directory\>/pipeline* directory for the Pipeline script, or go to the *<DataKit Installation Directory\>/python.d* directory for the Python script.

### Locally set Pipeline default script {#pipeline-settings}

[:octicons-tag-24: Version-1.61.0](changelog.md#cl-1.61.0)

Supports setting the default Pipeline script locally. If it conflicts with the default script set remotely, the local setting is preferred.

It can be configured in two ways:

- Host deployment, you can specify the default scripts for each category in the DataKit main configuration file, as follows:

    ```toml
    # default pipeline
    [pipeline.default_pipeline]
    # logging = "<your_script.p>"
    # metric = "<your_script.p>"
    # tracing = "<your_script.p>"
    ```

- Container deployment, you can use the environment variable, `ENV_PIPELINE_DEFAULT_PIPELINE`, its value is, for example, `{"logging":"abc.p","metric":"xyz.p"}`

### Set the Maximum Value of Open File Descriptor {#enable-max-fd}

In a Linux environment, you can configure the ulimit entry in the DataKit main configuration file to set the maximum number of open files for DataKit, as follows:

```toml
ulimit = 64000
```

ulimit is configured to 64000 by default.

<!-- markdownlint-disable MD013 -->
### CPU Utilization Rate Description for Resource Limit {#cgroup-how}
<!-- markdownlint-enable -->

CPU utilization is on a percentage basis (maximum 100.0). For an 8-core CPU, if the limit `cpu_max` is 20.0 (that is, 20%), the maximum CPU consumption of DataKit, will be displayed as about 160% on the top command.

### Collector Password Protection {#secrets_management}

[:octicons-tag-24: Version-1.31.0](changelog.md#cl-1.31.0)

If you wish to avoid storing passwords in plain text in configuration files, you can utilize this feature.

When DataKit loads the collector configuration file during startup and encounters `ENC[]`, it will replace the text with the password obtained from a file, environment variable, or AES encryption and reload it into memory to obtain the correct password.

ENC currently supports three methods:

- File Format (Recommended):

  Password format in the configuration file: `ENC[file:///path/to/enc4dk]`. Simply enter the correct password in the corresponding file.

- AES Encryption Method:

  You need to configure the secret key in the main configuration file `datakit.conf`: `crypto_AES_key` or `crypto_AES_Key_filePath`.
  The password should be formatted as: `ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]`

Here's an example using `mysql` to illustrate how to configure and use these methods:

1 File Format:

First, save the password in the file `/usr/local/datakit/enc4mysql`, then modify the configuration file mysql.conf:

```toml
# Partial configuration
[[inputs.mysql]]
  host = "localhost"
  user = "datakit"
  pass = "ENC[file:///usr/local/datakit/enc4mysql]"
  port = 3306
  # sock = "<SOCK>"
  # charset = "utf8"
```

DK will read the password from `/usr/local/datakit/enc4mysql` and replace it, resulting in `pass = "Hello*******"`

2 AES Encryption Method

First, configure the secret key in `datakit.conf`:

```toml
# Top-level field in the configuration file
# Secret key
crypto_AES_key = "0123456789abcdef"
# Or secret key file:
crypto_AES_Key_filePath = "/usr/local/datakit/mykey"
```

`mysql.conf` file:

```toml
pass = "ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]"
```

Note that the cipherText obtained through AES encryption needs to be filled in completely. Here is a code example：
<!-- markdownlint-disable MD046 -->
=== "Golang"

    ```go
    // AESEncrypt.
    func AESEncrypt(key []byte, plaintext string) (string, error) {
        block, err := aes.NewCipher(key)
        if err != nil {
            return "", err
        }
    
        // PKCS7 padding
        padding := aes.BlockSize - len(plaintext)%aes.BlockSize
        padtext := bytes.Repeat([]byte{byte(padding)}, padding)
        plaintext += string(padtext)
        ciphertext := make([]byte, aes.BlockSize+len(plaintext))
        iv := ciphertext[:aes.BlockSize]
        if _, err := io.ReadFull(rand.Reader, iv); err != nil {
            return "", err
        }
        mode := cipher.NewCBCEncrypter(block, iv)
        mode.CryptBlocks(ciphertext[aes.BlockSize:], []byte(plaintext))
    
        return base64.StdEncoding.EncodeToString(ciphertext), nil
    }
    
    // AESDecrypt AES.
    func AESDecrypt(key []byte, cryptoText string) (string, error) {
        ciphertext, err := base64.StdEncoding.DecodeString(cryptoText)
        if err != nil {
            return "", err
        }
    
        block, err := aes.NewCipher(key)
        if err != nil {
            return "", err
        }
    
        if len(ciphertext) < aes.BlockSize {
            return "", fmt.Errorf("ciphertext too short")
        }
    
        iv := ciphertext[:aes.BlockSize]
        ciphertext = ciphertext[aes.BlockSize:]
    
        mode := cipher.NewCBCDecrypter(block, iv)
        mode.CryptBlocks(ciphertext, ciphertext)
    
        // Remove PKCS7 padding
        padding := int(ciphertext[len(ciphertext)-1])
        if padding > aes.BlockSize {
            return "", fmt.Errorf("invalid padding")
        }
        ciphertext = ciphertext[:len(ciphertext)-padding]
    
        return string(ciphertext), nil
    }
    ```

=== "Java"

    ```java
    import javax.crypto.Cipher;
    import javax.crypto.spec.IvParameterSpec;
    import javax.crypto.spec.SecretKeySpec;
    import java.security.SecureRandom;
    import java.util.Base64;
    
    public class AESUtils {
        public static String AESEncrypt(byte[] key, String plaintext) throws Exception {
            javax.crypto.Cipher cipher = Cipher.getInstance("AES/CBC/PKCS5Padding");
            SecretKeySpec secretKeySpec = new SecretKeySpec(key, "AES");
    
            SecureRandom random = new SecureRandom();
            byte[] iv = new byte[16];
            random.nextBytes(iv);
            IvParameterSpec ivParameterSpec = new IvParameterSpec(iv);
            cipher.init(Cipher.ENCRYPT_MODE, secretKeySpec, ivParameterSpec);
            byte[] encrypted = cipher.doFinal(plaintext.getBytes());
            byte[] ivAndEncrypted = new byte[iv.length + encrypted.length];
            System.arraycopy(iv, 0, ivAndEncrypted, 0, iv.length);
            System.arraycopy(encrypted, 0, ivAndEncrypted, iv.length, encrypted.length);
    
            return Base64.getEncoder().encodeToString(ivAndEncrypted);
        }
    
        public static String AESDecrypt(byte[] key, String cryptoText) throws Exception {
            byte[] cipherText = Base64.getDecoder().decode(cryptoText);
    
            SecretKeySpec secretKeySpec = new SecretKeySpec(key, "AES");
    
            if (cipherText.length < 16) {
                throw new Exception("cipherText too short");
            }
    
            byte[] iv = new byte[16];
            System.arraycopy(cipherText, 0, iv, 0, 16);
            byte[] encrypted = new byte[cipherText.length - 16];
            System.arraycopy(cipherText, 16, encrypted, 0, cipherText.length - 16);
    
            Cipher cipher = Cipher.getInstance("AES/CBC/PKCS5Padding");
            IvParameterSpec ivParameterSpec = new IvParameterSpec(iv);
            cipher.init(Cipher.DECRYPT_MODE, secretKeySpec, ivParameterSpec);
    
            byte[] decrypted = cipher.doFinal(encrypted);
    
            return new String(decrypted);
        }
    }    
    ```
<!-- markdownlint-enable -->

In a K8S (Kubernetes) environment, private keys can be added through environment variables.
The environment variables ENV_CRYPTO_AES_KEY and ENV_CRYPTO_AES_KEY_FILEPATH can be referenced for this purpose:[DaemonSet 安装-其他](datakit-daemonset-deploy.md#env-others)

### Remote Job {#remote-job}

---

[:octicons-tag-24: Version-1.63.0](changelog.md#cl-1.63.0)

---

DataKit receives tasks dispatched from the center and executes them. Currently, it supports the `JVM dump` function.

It is command `jmap`,generate a jump file, and upload it to the `OSS`,`AWS S3 Bucket`, or `HuaWei Cloud OBS`.

After installing DataKit (DK), two files will be generated in the `template/service-task` directory of the installation folder: `jvm_dump_host_script.py` and `jvm_dump_k8s_script.py`.
The former is for the host machine mode, and the latter is for the virtual (Kubernetes) environment.

In the host machine environment, the current environment must have `python3` and the `requests` package installed. If not, you need to install it using:

```shell
pip install requests
# or
pip3 install requests

# if upload to OBS:
pip install esdk-obs-python --trusted-host pypi.org

# if upload to S3:
pip install boto3
```

The upload to multiple storage types can be controlled through environment variables. The following are configuration instructions, The k8s environment is the same:

```toml
# upload to OSS
[remote_job]
  enable = true
  envs = [
      "REMOTE=oss",
      "OSS_BUCKET_HOST=host","OSS_ACCESS_KEY_ID=key","OSS_ACCESS_KEY_SECRET=secret","OSS_BUCKET_NAME=bucket",
    ]
  interval = "30s"

# or upload to AWS:
[remote_job]
  enable = true
  envs = [
      "REMOTE=aws",
      "AWS_BUCKET_NAME=bucket","AWS_ACCESS_KEY_ID=AK","AWS_SECRET_ACCESS_KEY=SK","AWS_DEFAULT_REGION=us-west-2",
    ]
  interval = "30s"
  
# or upload to OBS:
[remote_job]
  enable = true
  envs = [
      "REMOTE=obs",
      "OBS_BUCKET_NAME=bucket","OBS_ACCESS_KEY_ID=AK","OBS_SECRET_ACCESS_KEY=SK","OBS_SERVER=https://xxx.myhuaweicloud.com"
    ]
  interval = "30s"    
```

In the Kubernetes (K8S) environment, access to the Kubernetes API is required, so Role-Based Access Control (RBAC) is necessary.

### Config {#config}


<!-- markdownlint-disable MD046 -->
=== "host"

    config dir:
    
    - Linux/Mac: `/usr/local/datakit/conf.d/datakit.conf`
    - Windows: `C:\Program Files\datakit\conf.d\datakit.conf`

    change conf:
    ```toml
    [remote_job]
      enable=true
      envs=["REMOTE=oss","OSS_BUCKET_HOST=<bucket_host>","OSS_ACCESS_KEY_ID=<key>","OSS_ACCESS_KEY_SECRET=<secret key>","OSS_BUCKET_NAME=<name>"]
      interval="100s"
      java_home=""
    ```

=== "Kubernetes"

    Add RBAC access:

    ```yaml

    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
    name: datakit
    rules:
    - apiGroups: ["rbac.authorization.k8s.io"]
      resources: ["clusterroles"]
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources: ["nodes", "nodes/stats", "nodes/metrics", "namespaces", "pods", "pods/log", "events", "services", "endpoints", "persistentvolumes", "persistentvolumeclaims", "pods/exec"]
      verbs: ["get", "list", "watch", "create"]
    - apiGroups: ["apps"]
      resources: ["deployments", "daemonsets", "statefulsets", "replicasets"]
      verbs: ["get", "list", "watch"]
    - apiGroups: ["batch"]
      resources: ["jobs", "cronjobs"]
      verbs: [ "get", "list", "watch"]
    - apiGroups: ["<<<custom_key.brand_main_domain>>>"]
      resources: ["datakits"]
      verbs: ["get","list"]
    - apiGroups: ["monitoring.coreos.com"]
      resources: ["podmonitors", "servicemonitors"]
      verbs: ["get", "list"]
    - apiGroups: ["metrics.k8s.io"]
      resources: ["pods", "nodes"]
      verbs: ["get", "list"]
    - nonResourceURLs: ["/metrics"]
      verbs: ["get"]
    
    ---
    ```

    In the above configuration, "pod/exec" is added, and the rest should be consistent with `datakit.yaml`.

    Add ENV for remote_job:

    ```yaml
    - name: ENV_REMOTE_JOB_ENABLE
      value: 'true'
    - name: ENV_REMOTE_JOB_ENVS
      value: >-
        REMOTE=oss,OSS_BUCKET_HOST=<bucket host>,OSS_ACCESS_KEY_ID=<key>,OSS_ACCESS_KEY_SECRET=<secret key>,OSS_BUCKET_NAME=<name>
    - name: ENV_REMOTE_JOB_JAVA_HOME
    - name: ENV_REMOTE_JOB_INTERVAL
      value: 100s

    ```

<!-- markdownlint-enable -->

Configuration file description:

1. `enable  ENV_REMOTE_JOB_ENABLE remote_job` Function switch.
2. `envs  ENV_REMOTE_JOB_ENVS` OSS configuration, including OSS `host` `access key` `secret key` `bucket` information, and send the obtained JVM dump file to OSS.
3. `interval ENV_REMOTE_JOB_INTERVAL` The time interval at which DataKit actively calls the interface to obtain the latest tasks.
4. `java_home ENV_REMOTE_JOB_JAVA_HOME` The host environment is automatically obtained from the environment variable ($JAVA_HOME) and does not need to be configured.

> Please note that the version of the Agent: `dd-java-agent.jar` used should not be lower than `v1.4.0-guance`.

### Point Pool {#point-pool}

[:octicons-tag-24: Version-1.28.0](changelog.md#cl-1.28.0) ·
[:octicons-beaker-24: Experimental](index.md#experimental)

> Point pool proved to be slow performance, do not enable it for production.

To optimize DataKit's memory usage under high load conditions, we can enable *Point Pool* to alleviate the pressure:

```toml
# datakit.conf
[point_pool]
    enable = true
    reserved_capacity = 4096
```

We can also enable `content_encoding = "v2"`([:octicons-tag-24: Version-1.32.0](changelog.md#cl-1.32.0) has enabled v2 by default) under [Dataway configure](datakit-conf.md#dataway-settings), with v2 encoding, it has lower memory and CPU overhead compared to v1.

<!-- markdownlint-disable MD046 -->
???+ attention

    While DataKit under low load(with a memory footprint of around), enable Point-Pool will eat more memory(we need more memory to cache unused data), but not excessively. The term "high load" typically refer to scenarios where memory consumption reach to 2GB or more. Enabling Point-Pool not only helps to memory usage but also improves DataKit's CPU consumption.
<!-- markdownlint-enable -->

## Extended Readings {#more-reading}

<font size=3>
<div class="grid cards" markdown>
- [<font color="coral"> :fontawesome-solid-arrow-right-long: &nbsp; <u>Host installation</u>: Install DataKit on host</font>](datakit-install.md)
</div>

<div class="grid cards" markdown>
- [<font color="coral"> :fontawesome-solid-arrow-right-long: &nbsp; <u>Kubernetes installation</u>: Install DataKit as DaemonSet</font>](datakit-daemonset-deploy.md)
</div>
</font>
